server:
  port: 3001
  host: "0.0.0.0"
  mode: "development" # development, production

database:
  path: "./data/pma.db"
  migrations_path: "./migrations"
  backup_enabled: true
  backup_path: "./data/backups"
  max_connections: 25

auth:
  jwt_secret: "your-secret-key-here" # Override with environment variable
  token_expiry: 3600 # seconds

home_assistant:
  url: "http://192.168.100.2:8123"
  token: "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJiYzkzMGUwZThlZmY0ZWEyOTc3NmI3MTIwYjc2NTAyYyIsImlhdCI6MTc1MjYxMTUxOSwiZXhwIjoyMDY3OTcxNTE5fQ.Dilak2Vad3GpSnRfrFBkAK3TRSUVQ42uOI4DMORJGoc" # Override with environment variable
  sync:
    enabled: true
    full_sync_interval: "1h"
    supported_domains: ["light", "switch", "sensor", "binary_sensor", "climate", "cover", "lock", "alarm_control_panel"]
    conflict_resolution: "homeassistant_wins"
    batch_size: 100
    retry_attempts: 3
    retry_delay: "5s"
    event_buffer_size: 1000
    event_processing_delay: "100ms"

logging:
  level: "info" # debug, info, warn, error
  format: "json" # json, text

websocket:
  ping_interval: 30 # seconds
  pong_timeout: 60 # seconds
  write_timeout: 10 # seconds

ai:
  fallback_enabled: true
  fallback_delay: "2s"
  default_provider: "ollama"
  max_retries: 3
  timeout: "30s"
  providers:
    - type: "ollama"
      enabled: true
      url: "http://localhost:11434"
      default_model: "llama2"
      auto_start: true
      priority: 1
      resource_limits:
        max_memory: "4GB"
        max_cpu: 80
    - type: "openai"
      enabled: false
      api_key: "" # Override with OPENAI_API_KEY environment variable
      default_model: "gpt-3.5-turbo"
      max_tokens: 4096
      priority: 2
    - type: "claude"
      enabled: false
      api_key: "" # Override with CLAUDE_API_KEY environment variable
      default_model: "claude-3-haiku-20240307"
      max_tokens: 4096
      priority: 3
    - type: "gemini"
      enabled: false
      api_key: "" # Override with GEMINI_API_KEY environment variable
      default_model: "gemini-pro"
      max_tokens: 4096
      priority: 4 